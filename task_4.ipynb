{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ne5Markup(id='001', text='Россия рассчитывает на конструктивное воздействие США на Грузию\\n\\n04/08/2008 12:08\\n\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\n\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\n\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ', spans=[Ne5Span(index='T1', type='GEOPOLIT', start=0, stop=6, text='Россия'), Ne5Span(index='T2', type='GEOPOLIT', start=50, stop=53, text='США'), Ne5Span(index='T3', type='GEOPOLIT', start=57, stop=63, text='Грузию'), Ne5Span(index='T4', type='LOC', start=87, stop=93, text='МОСКВА'), Ne5Span(index='T5', type='MEDIA', start=103, stop=114, text='РИА Новости'), Ne5Span(index='T6', type='GEOPOLIT', start=116, stop=122, text='Россия'), Ne5Span(index='T7', type='GEOPOLIT', start=141, stop=144, text='США'), Ne5Span(index='T8', type='GEOPOLIT', start=161, stop=168, text='Тбилиси'), Ne5Span(index='T9', type='GEOPOLIT', start=301, stop=307, text='России'), Ne5Span(index='T10', type='PER', start=308, stop=324, text='Григорий Карасин'), Ne5Span(index='T11', type='GEOPOLIT', start=383, stop=386, text='США'), Ne5Span(index='T12', type='PER', start=387, stop=402, text='Дэниэлом Фридом'), Ne5Span(index='T13', type='GEOPOLIT', start=505, stop=517, text='Южной Осетии'), Ne5Span(index='T14', type='GEOPOLIT', start=703, stop=709, text='Россия'), Ne5Span(index='T15', type='GEOPOLIT', start=723, stop=730, text='Тбилиси'), Ne5Span(index='T16', type='GEOPOLIT', start=815, stop=825, text='Вашингтона'), Ne5Span(index='T17', type='ORG', start=838, stop=841, text='МИД'), Ne5Span(index='T18', type='GEOPOLIT', start=842, stop=848, text='России')])\n"
     ]
    }
   ],
   "source": [
    "from corus import load_ne5\n",
    "import corus.sources.ne5 as ne5\n",
    "\n",
    "def load_text_utf8(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "ne5.load_text = load_text_utf8\n",
    "\n",
    "obj = load_ne5(\"Collection5\")\n",
    "\n",
    "for i, o in enumerate(obj):\n",
    "    if i == 0:\n",
    "        print(o)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2392 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "label_list=['O', 'B-GEOPOLIT', 'I-GEOPOLIT', 'B-LOC', 'I-LOC', 'B-MEDIA', 'I-MEDIA', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "from corus import load_ne5\n",
    "import corus.sources.ne5 as ne5\n",
    "import re\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_text_utf8(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "ne5.load_text = load_text_utf8\n",
    "obj = load_ne5(\"Collection5\")\n",
    "docs = list(obj)\n",
    "\n",
    "# Собираем все типы сущностей\n",
    "types = set()\n",
    "for doc in docs:\n",
    "    for span in doc.spans:\n",
    "        types.add(span.type)\n",
    "\n",
    "label_list = ['O']\n",
    "for entity_type in sorted(types):\n",
    "    label_list.extend([f'B-{entity_type}', f'I-{entity_type}'])\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "def process_doc(doc):\n",
    "    text = doc.text\n",
    "    spans = sorted(doc.spans, key=lambda x: x.stop - x.start, reverse=True)  # Приоритет длинным\n",
    "    \n",
    "    # Токенизация\n",
    "    tokenized = tokenizer(\n",
    "        text, \n",
    "        return_offsets_mapping=True,\n",
    "        truncation=False,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    tokens = []\n",
    "    for token, offset in zip(tokenized.tokens(), tokenized.offset_mapping):\n",
    "        if token in tokenizer.all_special_tokens:\n",
    "            continue\n",
    "        tokens.append({\n",
    "            'token': token,\n",
    "            'start': offset[0],\n",
    "            'end': offset[1],\n",
    "            'is_word_start': not token.startswith('##')\n",
    "        })\n",
    "    \n",
    "    # Инициализация меток\n",
    "    token_labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Обработка спанов в порядке приоритета\n",
    "    for span in spans:\n",
    "        span_type = span.type\n",
    "        span_start = span.start\n",
    "        span_end = span.stop\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token_labels[i] != 'O':\n",
    "                continue  # Уже занято более приоритетной сущностью\n",
    "                \n",
    "            if token['start'] >= span_end or token['end'] <= span_start:\n",
    "                continue\n",
    "                \n",
    "            # Определение B/I\n",
    "            prev_token = tokens[i-1] if i > 0 else None\n",
    "            if prev_token and (prev_token['end'] > span_start):\n",
    "                prefix = 'I-'\n",
    "            else:\n",
    "                prefix = 'B-'\n",
    "                \n",
    "            if token['is_word_start']:\n",
    "                token_labels[i] = f'{prefix}{span_type}'\n",
    "            else:\n",
    "                token_labels[i] = f'I-{span_type}'\n",
    "    \n",
    "    return {\n",
    "        'tokens': [t['token'] for t in tokens],\n",
    "        'ner_tags': [label2id[label] for label in token_labels]\n",
    "    }\n",
    "\n",
    "# Обрабатываем все документы\n",
    "processed_data = [process_doc(doc) for doc in docs]\n",
    "\n",
    "# Создаем Dataset\n",
    "dataset = Dataset.from_list(processed_data)\n",
    "dataset.features['ner_tags'].feature.names = label_list\n",
    "\n",
    "print(dataset)\n",
    "print(f\"{label_list=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf06d0236950487fb5b7dbd6913e7c7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e881e59815824de9b1480b51cdbf6bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ДО дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     2.4641 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0027 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.0028 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.0193 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.0049 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.0680 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.7206 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   277.5540 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    18.0410 |\n",
      "+-----------------------------+------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5100' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5100/10000 04:34 < 04:23, 18.59 it/s, Epoch 102/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.318300</td>\n",
       "      <td>0.889212</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.110502</td>\n",
       "      <td>0.069733</td>\n",
       "      <td>0.085506</td>\n",
       "      <td>0.738903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.791500</td>\n",
       "      <td>0.636209</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.213055</td>\n",
       "      <td>0.201780</td>\n",
       "      <td>0.207264</td>\n",
       "      <td>0.799799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.613100</td>\n",
       "      <td>0.512768</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.277874</td>\n",
       "      <td>0.318002</td>\n",
       "      <td>0.296587</td>\n",
       "      <td>0.845246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.505000</td>\n",
       "      <td>0.436167</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.339482</td>\n",
       "      <td>0.382295</td>\n",
       "      <td>0.359619</td>\n",
       "      <td>0.867891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.428800</td>\n",
       "      <td>0.388828</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.383893</td>\n",
       "      <td>0.424332</td>\n",
       "      <td>0.403101</td>\n",
       "      <td>0.880271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.371500</td>\n",
       "      <td>0.356753</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.422113</td>\n",
       "      <td>0.466370</td>\n",
       "      <td>0.443139</td>\n",
       "      <td>0.889953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.329600</td>\n",
       "      <td>0.337679</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.428252</td>\n",
       "      <td>0.473788</td>\n",
       "      <td>0.449871</td>\n",
       "      <td>0.893498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.294800</td>\n",
       "      <td>0.324441</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.450749</td>\n",
       "      <td>0.491098</td>\n",
       "      <td>0.470059</td>\n",
       "      <td>0.898154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.266600</td>\n",
       "      <td>0.315963</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.452158</td>\n",
       "      <td>0.502473</td>\n",
       "      <td>0.475990</td>\n",
       "      <td>0.900693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.244100</td>\n",
       "      <td>0.308002</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.458707</td>\n",
       "      <td>0.505440</td>\n",
       "      <td>0.480941</td>\n",
       "      <td>0.902174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.223900</td>\n",
       "      <td>0.303256</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.472498</td>\n",
       "      <td>0.518299</td>\n",
       "      <td>0.494340</td>\n",
       "      <td>0.904238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.206500</td>\n",
       "      <td>0.302016</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.478709</td>\n",
       "      <td>0.528190</td>\n",
       "      <td>0.502234</td>\n",
       "      <td>0.905825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.300783</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.489842</td>\n",
       "      <td>0.536597</td>\n",
       "      <td>0.512155</td>\n",
       "      <td>0.906672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.176000</td>\n",
       "      <td>0.302578</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.482682</td>\n",
       "      <td>0.537587</td>\n",
       "      <td>0.508657</td>\n",
       "      <td>0.907359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.163000</td>\n",
       "      <td>0.299317</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.493938</td>\n",
       "      <td>0.544016</td>\n",
       "      <td>0.517769</td>\n",
       "      <td>0.908788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.151700</td>\n",
       "      <td>0.301235</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.492163</td>\n",
       "      <td>0.543521</td>\n",
       "      <td>0.516569</td>\n",
       "      <td>0.908365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.143300</td>\n",
       "      <td>0.301939</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.488434</td>\n",
       "      <td>0.543027</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>0.908629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.135600</td>\n",
       "      <td>0.303219</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.499776</td>\n",
       "      <td>0.551434</td>\n",
       "      <td>0.524336</td>\n",
       "      <td>0.910957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.127200</td>\n",
       "      <td>0.303631</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.497092</td>\n",
       "      <td>0.549456</td>\n",
       "      <td>0.521964</td>\n",
       "      <td>0.911486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.120600</td>\n",
       "      <td>0.308506</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.503626</td>\n",
       "      <td>0.549456</td>\n",
       "      <td>0.525544</td>\n",
       "      <td>0.911380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.113500</td>\n",
       "      <td>0.311274</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.491979</td>\n",
       "      <td>0.545994</td>\n",
       "      <td>0.517581</td>\n",
       "      <td>0.910745</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.106700</td>\n",
       "      <td>0.315732</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.511861</td>\n",
       "      <td>0.554896</td>\n",
       "      <td>0.532511</td>\n",
       "      <td>0.912121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.101300</td>\n",
       "      <td>0.316302</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.506318</td>\n",
       "      <td>0.554896</td>\n",
       "      <td>0.529495</td>\n",
       "      <td>0.913126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.094800</td>\n",
       "      <td>0.323132</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.498452</td>\n",
       "      <td>0.557369</td>\n",
       "      <td>0.526267</td>\n",
       "      <td>0.912544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.326867</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.509238</td>\n",
       "      <td>0.558853</td>\n",
       "      <td>0.532893</td>\n",
       "      <td>0.912121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.327065</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.509707</td>\n",
       "      <td>0.558358</td>\n",
       "      <td>0.532924</td>\n",
       "      <td>0.912597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.084200</td>\n",
       "      <td>0.330498</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.511712</td>\n",
       "      <td>0.561820</td>\n",
       "      <td>0.535596</td>\n",
       "      <td>0.912280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.333512</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.514493</td>\n",
       "      <td>0.561820</td>\n",
       "      <td>0.537116</td>\n",
       "      <td>0.912862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.075800</td>\n",
       "      <td>0.339381</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.560831</td>\n",
       "      <td>0.533898</td>\n",
       "      <td>0.912015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.072400</td>\n",
       "      <td>0.341895</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.513223</td>\n",
       "      <td>0.566271</td>\n",
       "      <td>0.538443</td>\n",
       "      <td>0.912862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.343580</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.510962</td>\n",
       "      <td>0.564787</td>\n",
       "      <td>0.536528</td>\n",
       "      <td>0.912650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.066500</td>\n",
       "      <td>0.344781</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.500894</td>\n",
       "      <td>0.553907</td>\n",
       "      <td>0.526069</td>\n",
       "      <td>0.911275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.353787</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.508072</td>\n",
       "      <td>0.560336</td>\n",
       "      <td>0.532926</td>\n",
       "      <td>0.911909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.354998</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.512775</td>\n",
       "      <td>0.565776</td>\n",
       "      <td>0.537973</td>\n",
       "      <td>0.912333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.058100</td>\n",
       "      <td>0.359436</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.512958</td>\n",
       "      <td>0.567755</td>\n",
       "      <td>0.538967</td>\n",
       "      <td>0.912121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.056000</td>\n",
       "      <td>0.363400</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.511021</td>\n",
       "      <td>0.561820</td>\n",
       "      <td>0.535218</td>\n",
       "      <td>0.912597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.054800</td>\n",
       "      <td>0.364721</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.506442</td>\n",
       "      <td>0.563798</td>\n",
       "      <td>0.533583</td>\n",
       "      <td>0.912544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.369340</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.516390</td>\n",
       "      <td>0.568744</td>\n",
       "      <td>0.541304</td>\n",
       "      <td>0.912491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.051300</td>\n",
       "      <td>0.368858</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.518585</td>\n",
       "      <td>0.572700</td>\n",
       "      <td>0.544301</td>\n",
       "      <td>0.912386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.049500</td>\n",
       "      <td>0.374652</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.514989</td>\n",
       "      <td>0.569238</td>\n",
       "      <td>0.540756</td>\n",
       "      <td>0.912809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.046600</td>\n",
       "      <td>0.378530</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.522911</td>\n",
       "      <td>0.575668</td>\n",
       "      <td>0.548023</td>\n",
       "      <td>0.913073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>0.046200</td>\n",
       "      <td>0.380712</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.517504</td>\n",
       "      <td>0.570227</td>\n",
       "      <td>0.542588</td>\n",
       "      <td>0.912703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>0.044300</td>\n",
       "      <td>0.384247</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.516734</td>\n",
       "      <td>0.572700</td>\n",
       "      <td>0.543279</td>\n",
       "      <td>0.912491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.389962</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.522167</td>\n",
       "      <td>0.576657</td>\n",
       "      <td>0.548061</td>\n",
       "      <td>0.912280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.042100</td>\n",
       "      <td>0.388830</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.509813</td>\n",
       "      <td>0.565282</td>\n",
       "      <td>0.536116</td>\n",
       "      <td>0.911645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.393869</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.513598</td>\n",
       "      <td>0.569733</td>\n",
       "      <td>0.540211</td>\n",
       "      <td>0.912068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>0.038900</td>\n",
       "      <td>0.397833</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.512900</td>\n",
       "      <td>0.570227</td>\n",
       "      <td>0.540047</td>\n",
       "      <td>0.911909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.397720</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.515869</td>\n",
       "      <td>0.570722</td>\n",
       "      <td>0.541911</td>\n",
       "      <td>0.912174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>0.037900</td>\n",
       "      <td>0.398099</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.516302</td>\n",
       "      <td>0.571711</td>\n",
       "      <td>0.542596</td>\n",
       "      <td>0.912386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.035700</td>\n",
       "      <td>0.404227</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.519568</td>\n",
       "      <td>0.571217</td>\n",
       "      <td>0.544170</td>\n",
       "      <td>0.912386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>0.035500</td>\n",
       "      <td>0.408981</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.510449</td>\n",
       "      <td>0.567755</td>\n",
       "      <td>0.537579</td>\n",
       "      <td>0.912438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ПОСЛЕ дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     0.3419 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0027 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.5132 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.5663 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.5384 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.9129 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.5007 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   399.4760 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    25.9660 |\n",
      "+-----------------------------+------------+\n",
      "| epoch                       |   102.0000 |\n",
      "+-----------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "splitted_dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42) if not isinstance(dataset, DatasetDict) else dataset\n",
    "tokenized_dataset = splitted_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=splitted_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(\n",
    "        predictions=true_predictions,\n",
    "        references=true_labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "# Конфигурация обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    max_steps=10000,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "# Дообучение\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,\n",
    "    early_stopping_threshold=0.001,\n",
    "))\n",
    "\n",
    "baseline_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ДО дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    baseline_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "final_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ПОСЛЕ дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    final_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf7deb5a31244c7e8b2774dc3990d559",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Разделение данных на train и test\n",
    "train_docs, test_docs = train_test_split(docs, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "# Подготовка данных для MLM\n",
    "mlm_texts = [doc.text for doc in train_docs]\n",
    "mlm_dataset = Dataset.from_dict({\"text\": mlm_texts})\n",
    "\n",
    "def tokenize_mlm(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_mlm = mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Обучение MLM\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "data_collator_mlm = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=\"./mlm_results\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./mlm_logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_mlm = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=tokenized_mlm,\n",
    "    data_collator=data_collator_mlm\n",
    ")\n",
    "\n",
    "trainer_mlm.train()\n",
    "mlm_model.save_pretrained(\"./mlm_model\")\n",
    "\n",
    "\n",
    "\n",
    "# Создаем Dataset\n",
    "dataset = Dataset.from_list(processed_data)\n",
    "dataset.features['ner_tags'].feature.names = label_list\n",
    "splitted_dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "tokenized_dataset = splitted_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=splitted_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Инициализация модели для NER\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"./mlm_model\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Обучение NER\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    max_steps =10000,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,\n",
    "    early_stopping_threshold=0.001\n",
    "))\n",
    "\n",
    "baseline_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ДО дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    baseline_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "final_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ПОСЛЕ дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    final_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
