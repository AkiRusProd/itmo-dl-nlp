{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ne5Markup(id='001', text='Россия рассчитывает на конструктивное воздействие США на Грузию\\n\\n04/08/2008 12:08\\n\\nМОСКВА, 4 авг - РИА Новости. Россия рассчитывает, что США воздействуют на Тбилиси в связи с обострением ситуации в зоне грузино-осетинского конфликта. Об этом статс-секретарь - заместитель министра иностранных дел России Григорий Карасин заявил в телефонном разговоре с заместителем госсекретаря США Дэниэлом Фридом.\\n\\n\"С российской стороны выражена глубокая озабоченность в связи с новым витком напряженности вокруг Южной Осетии, противозаконными действиями грузинской стороны по наращиванию своих вооруженных сил в регионе, бесконтрольным строительством фортификационных сооружений\", - говорится в сообщении.\\n\\n\"Россия уже призвала Тбилиси к ответственной линии и рассчитывает также на конструктивное воздействие со стороны Вашингтона\", - сообщил МИД России. ', spans=[Ne5Span(index='T1', type='GEOPOLIT', start=0, stop=6, text='Россия'), Ne5Span(index='T2', type='GEOPOLIT', start=50, stop=53, text='США'), Ne5Span(index='T3', type='GEOPOLIT', start=57, stop=63, text='Грузию'), Ne5Span(index='T4', type='LOC', start=87, stop=93, text='МОСКВА'), Ne5Span(index='T5', type='MEDIA', start=103, stop=114, text='РИА Новости'), Ne5Span(index='T6', type='GEOPOLIT', start=116, stop=122, text='Россия'), Ne5Span(index='T7', type='GEOPOLIT', start=141, stop=144, text='США'), Ne5Span(index='T8', type='GEOPOLIT', start=161, stop=168, text='Тбилиси'), Ne5Span(index='T9', type='GEOPOLIT', start=301, stop=307, text='России'), Ne5Span(index='T10', type='PER', start=308, stop=324, text='Григорий Карасин'), Ne5Span(index='T11', type='GEOPOLIT', start=383, stop=386, text='США'), Ne5Span(index='T12', type='PER', start=387, stop=402, text='Дэниэлом Фридом'), Ne5Span(index='T13', type='GEOPOLIT', start=505, stop=517, text='Южной Осетии'), Ne5Span(index='T14', type='GEOPOLIT', start=703, stop=709, text='Россия'), Ne5Span(index='T15', type='GEOPOLIT', start=723, stop=730, text='Тбилиси'), Ne5Span(index='T16', type='GEOPOLIT', start=815, stop=825, text='Вашингтона'), Ne5Span(index='T17', type='ORG', start=838, stop=841, text='МИД'), Ne5Span(index='T18', type='GEOPOLIT', start=842, stop=848, text='России')])\n"
     ]
    }
   ],
   "source": [
    "from corus import load_ne5\n",
    "import corus.sources.ne5 as ne5\n",
    "\n",
    "def load_text_utf8(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "ne5.load_text = load_text_utf8\n",
    "\n",
    "obj = load_ne5(\"Collection5\")\n",
    "\n",
    "for i, o in enumerate(obj):\n",
    "    if i == 0:\n",
    "        print(o)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка и подготовка набора данных Collection5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2392 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "{'tokens': ['Россия', 'рассчитывает', 'на', 'конструктивно', '##е', 'воздействие', 'США', 'на', 'Грузию', '04', '/', '08', '/', '2008', '12', ':', '08', 'МО', '##СК', '##ВА', ',', '4', 'ав', '##г', '-', 'РИА', 'Новости', '.', 'Россия', 'рассчитывает', ',', 'что', 'США', 'воздейс', '##твуют', 'на', 'Тбилиси', 'в', 'связи', 'с', 'обострение', '##м', 'ситуации', 'в', 'зоне', 'груз', '##ино', '-', 'осети', '##нского', 'конфликта', '.', 'Об', 'этом', 'стат', '##с', '-', 'секретарь', '-', 'заместитель', 'министра', 'иностранных', 'дел', 'России', 'Григорий', 'Кара', '##син', 'заявил', 'в', 'телефон', '##ном', 'разговоре', 'с', 'заместителем', 'госсекретаря', 'США', 'Дэни', '##эл', '##ом', 'Фрид', '##ом', '.', '\"', 'С', 'российской', 'стороны', 'выражена', 'глубокая', 'озабоченность', 'в', 'связи', 'с', 'новым', 'ви', '##тком', 'напряженности', 'вокруг', 'Южной', 'Осетии', ',', 'противо', '##зак', '##онными', 'действиями', 'грузинской', 'стороны', 'по', 'наращи', '##ванию', 'своих', 'вооруженных', 'сил', 'в', 'регионе', ',', 'бесконтро', '##льным', 'строительством', 'форт', '##ифика', '##ционных', 'сооружений', '\"', ',', '-', 'говорится', 'в', 'сообщении', '.', '\"', 'Россия', 'уже', 'призвала', 'Тбилиси', 'к', 'ответственно', '##й', 'линии', 'и', 'рассчитывает', 'также', 'на', 'конструктивно', '##е', 'воздействие', 'со', 'стороны', 'Вашингтона', '\"', ',', '-', 'сообщил', 'МИД', 'России', '.'], 'ner_tags': [1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 0, 0, 0, 0, 5, 6, 6, 2, 0, 0, 0, 1, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 9, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 1, 10, 10, 10, 10, 10, 10, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 0, 7, 8]}\n",
      "label_list=['O', 'B-GEOPOLIT', 'I-GEOPOLIT', 'B-LOC', 'I-LOC', 'B-MEDIA', 'I-MEDIA', 'B-ORG', 'I-ORG', 'B-PER', 'I-PER']\n"
     ]
    }
   ],
   "source": [
    "from corus import load_ne5\n",
    "import corus.sources.ne5 as ne5\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def load_text_utf8(path):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return f.read()\n",
    "\n",
    "ne5.load_text = load_text_utf8\n",
    "obj = load_ne5(\"Collection5\")\n",
    "docs = list(obj)\n",
    "\n",
    "# Собираем все типы сущностей\n",
    "types = set()\n",
    "for doc in docs:\n",
    "    for span in doc.spans:\n",
    "        types.add(span.type)\n",
    "\n",
    "label_list = ['O']\n",
    "for entity_type in sorted(types):\n",
    "    label_list.extend([f'B-{entity_type}', f'I-{entity_type}'])\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "def process_doc(doc):\n",
    "    text = doc.text\n",
    "    spans = sorted(doc.spans, key=lambda x: x.stop - x.start, reverse=True)  # Приоритет длинным\n",
    "    \n",
    "    # Токенизация\n",
    "    tokenized = tokenizer(\n",
    "        text, \n",
    "        return_offsets_mapping=True,\n",
    "        truncation=False,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    tokens = []\n",
    "    for token, offset in zip(tokenized.tokens(), tokenized.offset_mapping):\n",
    "        if token in tokenizer.all_special_tokens:\n",
    "            continue\n",
    "        tokens.append({\n",
    "            'token': token,\n",
    "            'start': offset[0],\n",
    "            'end': offset[1],\n",
    "            'is_word_start': not token.startswith('##')\n",
    "        })\n",
    "    \n",
    "    # Инициализация меток\n",
    "    token_labels = ['O'] * len(tokens)\n",
    "    \n",
    "    # Обработка спанов в порядке приоритета\n",
    "    for span in spans:\n",
    "        span_type = span.type\n",
    "        span_start = span.start\n",
    "        span_end = span.stop\n",
    "        \n",
    "        for i, token in enumerate(tokens):\n",
    "            if token_labels[i] != 'O':\n",
    "                continue  # Уже занято более приоритетной сущностью\n",
    "                \n",
    "            if token['start'] >= span_end or token['end'] <= span_start:\n",
    "                continue\n",
    "                \n",
    "            # Определение B/I\n",
    "            prev_token = tokens[i-1] if i > 0 else None\n",
    "            if prev_token and (prev_token['end'] > span_start):\n",
    "                prefix = 'I-'\n",
    "            else:\n",
    "                prefix = 'B-'\n",
    "                \n",
    "            if token['is_word_start']:\n",
    "                token_labels[i] = f'{prefix}{span_type}'\n",
    "            else:\n",
    "                token_labels[i] = f'I-{span_type}'\n",
    "    \n",
    "    return {\n",
    "        'tokens': [t['token'] for t in tokens],\n",
    "        'ner_tags': [label2id[label] for label in token_labels]\n",
    "    }\n",
    "\n",
    "# Обрабатываем все документы\n",
    "processed_data = [process_doc(doc) for doc in docs]\n",
    "\n",
    "# Создаем Dataset\n",
    "dataset = Dataset.from_list(processed_data)\n",
    "dataset.features['ner_tags'].feature.names = label_list\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[0])\n",
    "print(f\"{label_list=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод `compute_metrics` для вычисления метрик на основе предсказаний модели и истинных меток."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for l in label if l != -100]\n",
    "        for label in labels\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(\n",
    "        predictions=true_predictions,\n",
    "        references=true_labels,\n",
    "        zero_division=0\n",
    "    )\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дообучение модели rubert-tiny2 на train-части корпуса для решения NER-задачи и замеры качества NER-метрик до и после дообучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "652a26bd356740089700a2c239a1596c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "274e0baecfb44c778065e9d469eefada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ДО дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     2.5222 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0010 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.0062 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.0475 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.0110 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.0502 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.7425 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   269.3460 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    17.5070 |\n",
      "+-----------------------------+------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4100' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4100/10000 03:45 < 05:24, 18.21 it/s, Epoch 82/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.352200</td>\n",
       "      <td>0.923296</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.072864</td>\n",
       "      <td>0.043027</td>\n",
       "      <td>0.054104</td>\n",
       "      <td>0.730649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.806400</td>\n",
       "      <td>0.650835</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.215002</td>\n",
       "      <td>0.222552</td>\n",
       "      <td>0.218712</td>\n",
       "      <td>0.799958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.623600</td>\n",
       "      <td>0.521369</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.267760</td>\n",
       "      <td>0.315035</td>\n",
       "      <td>0.289480</td>\n",
       "      <td>0.844135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.513500</td>\n",
       "      <td>0.443436</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.329834</td>\n",
       "      <td>0.372898</td>\n",
       "      <td>0.350046</td>\n",
       "      <td>0.864716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.434700</td>\n",
       "      <td>0.393364</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.427794</td>\n",
       "      <td>0.405057</td>\n",
       "      <td>0.880535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.376500</td>\n",
       "      <td>0.361805</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.412869</td>\n",
       "      <td>0.456973</td>\n",
       "      <td>0.433803</td>\n",
       "      <td>0.889688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.332800</td>\n",
       "      <td>0.341149</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.427995</td>\n",
       "      <td>0.471810</td>\n",
       "      <td>0.448836</td>\n",
       "      <td>0.893339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.297200</td>\n",
       "      <td>0.327853</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.443596</td>\n",
       "      <td>0.488131</td>\n",
       "      <td>0.464799</td>\n",
       "      <td>0.897360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.269100</td>\n",
       "      <td>0.320625</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.451512</td>\n",
       "      <td>0.501978</td>\n",
       "      <td>0.475410</td>\n",
       "      <td>0.899053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.246500</td>\n",
       "      <td>0.312153</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.468807</td>\n",
       "      <td>0.512859</td>\n",
       "      <td>0.489844</td>\n",
       "      <td>0.901963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.226200</td>\n",
       "      <td>0.307291</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.464286</td>\n",
       "      <td>0.507913</td>\n",
       "      <td>0.485120</td>\n",
       "      <td>0.902333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.207800</td>\n",
       "      <td>0.303416</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.469793</td>\n",
       "      <td>0.515331</td>\n",
       "      <td>0.491509</td>\n",
       "      <td>0.903391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.193100</td>\n",
       "      <td>0.303553</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.480108</td>\n",
       "      <td>0.525223</td>\n",
       "      <td>0.501653</td>\n",
       "      <td>0.905190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.177000</td>\n",
       "      <td>0.304323</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.473070</td>\n",
       "      <td>0.521266</td>\n",
       "      <td>0.496000</td>\n",
       "      <td>0.905984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.164500</td>\n",
       "      <td>0.300790</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.487098</td>\n",
       "      <td>0.532146</td>\n",
       "      <td>0.508627</td>\n",
       "      <td>0.907518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.153500</td>\n",
       "      <td>0.303768</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.485779</td>\n",
       "      <td>0.532146</td>\n",
       "      <td>0.507907</td>\n",
       "      <td>0.908100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.145100</td>\n",
       "      <td>0.303006</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.484031</td>\n",
       "      <td>0.532146</td>\n",
       "      <td>0.506949</td>\n",
       "      <td>0.907941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.136600</td>\n",
       "      <td>0.305847</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.492780</td>\n",
       "      <td>0.540059</td>\n",
       "      <td>0.515337</td>\n",
       "      <td>0.909158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.128200</td>\n",
       "      <td>0.305810</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.497052</td>\n",
       "      <td>0.542038</td>\n",
       "      <td>0.518571</td>\n",
       "      <td>0.909846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.120000</td>\n",
       "      <td>0.310369</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.498869</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>0.521143</td>\n",
       "      <td>0.910851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.113900</td>\n",
       "      <td>0.313109</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.491023</td>\n",
       "      <td>0.541048</td>\n",
       "      <td>0.514824</td>\n",
       "      <td>0.909952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.107300</td>\n",
       "      <td>0.316058</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.504758</td>\n",
       "      <td>0.550940</td>\n",
       "      <td>0.526838</td>\n",
       "      <td>0.910904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.102600</td>\n",
       "      <td>0.317582</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.506529</td>\n",
       "      <td>0.556380</td>\n",
       "      <td>0.530285</td>\n",
       "      <td>0.912121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.095700</td>\n",
       "      <td>0.322972</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.493512</td>\n",
       "      <td>0.545500</td>\n",
       "      <td>0.518205</td>\n",
       "      <td>0.910798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.328086</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.496611</td>\n",
       "      <td>0.543521</td>\n",
       "      <td>0.519008</td>\n",
       "      <td>0.910375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.087400</td>\n",
       "      <td>0.329917</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.493163</td>\n",
       "      <td>0.535114</td>\n",
       "      <td>0.513283</td>\n",
       "      <td>0.910322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.085300</td>\n",
       "      <td>0.331760</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.499548</td>\n",
       "      <td>0.546489</td>\n",
       "      <td>0.521965</td>\n",
       "      <td>0.910428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.079300</td>\n",
       "      <td>0.336447</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.506781</td>\n",
       "      <td>0.554402</td>\n",
       "      <td>0.529523</td>\n",
       "      <td>0.911909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.340799</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.499775</td>\n",
       "      <td>0.550445</td>\n",
       "      <td>0.523888</td>\n",
       "      <td>0.910640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.072000</td>\n",
       "      <td>0.340940</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.502708</td>\n",
       "      <td>0.550940</td>\n",
       "      <td>0.525720</td>\n",
       "      <td>0.910957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.070000</td>\n",
       "      <td>0.341664</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.512337</td>\n",
       "      <td>0.564787</td>\n",
       "      <td>0.537285</td>\n",
       "      <td>0.912121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.066100</td>\n",
       "      <td>0.347690</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.504456</td>\n",
       "      <td>0.559842</td>\n",
       "      <td>0.530708</td>\n",
       "      <td>0.911486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.351770</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.505583</td>\n",
       "      <td>0.559842</td>\n",
       "      <td>0.531331</td>\n",
       "      <td>0.911804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.061800</td>\n",
       "      <td>0.355813</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.507168</td>\n",
       "      <td>0.559842</td>\n",
       "      <td>0.532205</td>\n",
       "      <td>0.911275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.058300</td>\n",
       "      <td>0.357301</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.506244</td>\n",
       "      <td>0.561325</td>\n",
       "      <td>0.532364</td>\n",
       "      <td>0.912386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.056700</td>\n",
       "      <td>0.362627</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.503607</td>\n",
       "      <td>0.552423</td>\n",
       "      <td>0.526887</td>\n",
       "      <td>0.911222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.055400</td>\n",
       "      <td>0.364032</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.505159</td>\n",
       "      <td>0.556874</td>\n",
       "      <td>0.529758</td>\n",
       "      <td>0.912068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.053700</td>\n",
       "      <td>0.366595</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.502929</td>\n",
       "      <td>0.551929</td>\n",
       "      <td>0.526291</td>\n",
       "      <td>0.911857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.051600</td>\n",
       "      <td>0.369418</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.501559</td>\n",
       "      <td>0.556874</td>\n",
       "      <td>0.527771</td>\n",
       "      <td>0.911645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.049400</td>\n",
       "      <td>0.374534</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.504472</td>\n",
       "      <td>0.557864</td>\n",
       "      <td>0.529826</td>\n",
       "      <td>0.911327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.046800</td>\n",
       "      <td>0.377824</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.507149</td>\n",
       "      <td>0.561325</td>\n",
       "      <td>0.532864</td>\n",
       "      <td>0.911804</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ПОСЛЕ дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     0.3104 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0010 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.4989 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.5455 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.5211 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.9109 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.5073 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   394.2640 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    25.6270 |\n",
      "+-----------------------------+------------+\n",
      "| epoch                       |    82.0000 |\n",
      "+-----------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "import tabulate\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "splitted_dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42) if not isinstance(dataset, DatasetDict) else dataset\n",
    "tokenized_dataset = splitted_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=splitted_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Конфигурация обучения\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    max_steps=10000,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    # warmup_steps=500\n",
    ")\n",
    "\n",
    "# Дообучение\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,\n",
    "    early_stopping_threshold=0.001,\n",
    "))\n",
    "\n",
    "baseline_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ДО дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    baseline_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "final_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ПОСЛЕ дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    final_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем ориентироваться на F1-меру, потому что она учитывает как точность, так и полноту.     \n",
    "F1 до обучения: 0.0110      \n",
    "F1 после обучения: 0.5211       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительно дообучение на train-части в MLM режиме, а потом дообучение на NER-задачу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f16aaece9f8243409b80b2d6c0cc8d93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3000' max='3000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3000/3000 11:26, Epoch 60/60]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.859200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.448500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.241000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.104900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.032100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.970300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f21761e2934223b9806ca911c03aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2274979052459fb27d8a59a81f2b22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at ./mlm_model and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ДО дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     2.3312 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0010 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.0039 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.0242 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.0067 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.1649 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.7155 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   279.5100 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    18.1680 |\n",
      "+-----------------------------+------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4100' max='10000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4100/10000 03:38 < 05:13, 18.80 it/s, Epoch 82/200]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.330000</td>\n",
       "      <td>0.872380</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.085932</td>\n",
       "      <td>0.055885</td>\n",
       "      <td>0.067726</td>\n",
       "      <td>0.743770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.758100</td>\n",
       "      <td>0.594611</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.242105</td>\n",
       "      <td>0.238872</td>\n",
       "      <td>0.240478</td>\n",
       "      <td>0.818634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.577600</td>\n",
       "      <td>0.482423</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.305792</td>\n",
       "      <td>0.355094</td>\n",
       "      <td>0.328604</td>\n",
       "      <td>0.859531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.474800</td>\n",
       "      <td>0.412696</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.351614</td>\n",
       "      <td>0.393175</td>\n",
       "      <td>0.371235</td>\n",
       "      <td>0.875615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.402900</td>\n",
       "      <td>0.368551</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.401336</td>\n",
       "      <td>0.445598</td>\n",
       "      <td>0.422311</td>\n",
       "      <td>0.887837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.347800</td>\n",
       "      <td>0.342320</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.437190</td>\n",
       "      <td>0.480218</td>\n",
       "      <td>0.457695</td>\n",
       "      <td>0.895773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.308800</td>\n",
       "      <td>0.326711</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.450831</td>\n",
       "      <td>0.496538</td>\n",
       "      <td>0.472582</td>\n",
       "      <td>0.898312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.274800</td>\n",
       "      <td>0.313859</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.465635</td>\n",
       "      <td>0.505935</td>\n",
       "      <td>0.484949</td>\n",
       "      <td>0.901910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.248900</td>\n",
       "      <td>0.308543</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.460854</td>\n",
       "      <td>0.512364</td>\n",
       "      <td>0.485246</td>\n",
       "      <td>0.903920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.300886</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.522255</td>\n",
       "      <td>0.500237</td>\n",
       "      <td>0.905931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.208600</td>\n",
       "      <td>0.298225</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.487982</td>\n",
       "      <td>0.532146</td>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.907254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.191900</td>\n",
       "      <td>0.296204</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.495277</td>\n",
       "      <td>0.544510</td>\n",
       "      <td>0.518728</td>\n",
       "      <td>0.909158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.178100</td>\n",
       "      <td>0.295836</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.495684</td>\n",
       "      <td>0.539565</td>\n",
       "      <td>0.516694</td>\n",
       "      <td>0.910534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.163200</td>\n",
       "      <td>0.298857</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.497548</td>\n",
       "      <td>0.551929</td>\n",
       "      <td>0.523329</td>\n",
       "      <td>0.909476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.152400</td>\n",
       "      <td>0.296798</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.507727</td>\n",
       "      <td>0.552423</td>\n",
       "      <td>0.529133</td>\n",
       "      <td>0.911909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.140400</td>\n",
       "      <td>0.298675</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.507706</td>\n",
       "      <td>0.553907</td>\n",
       "      <td>0.529801</td>\n",
       "      <td>0.911592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.132600</td>\n",
       "      <td>0.297999</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.504274</td>\n",
       "      <td>0.554402</td>\n",
       "      <td>0.528151</td>\n",
       "      <td>0.911327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.124900</td>\n",
       "      <td>0.302675</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.513327</td>\n",
       "      <td>0.552423</td>\n",
       "      <td>0.532158</td>\n",
       "      <td>0.912121</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.118200</td>\n",
       "      <td>0.303222</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.512217</td>\n",
       "      <td>0.559842</td>\n",
       "      <td>0.534972</td>\n",
       "      <td>0.912650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.111300</td>\n",
       "      <td>0.308978</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.519639</td>\n",
       "      <td>0.569238</td>\n",
       "      <td>0.543309</td>\n",
       "      <td>0.912915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.104700</td>\n",
       "      <td>0.310402</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.511461</td>\n",
       "      <td>0.562809</td>\n",
       "      <td>0.535908</td>\n",
       "      <td>0.912703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.099300</td>\n",
       "      <td>0.315707</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.525193</td>\n",
       "      <td>0.572206</td>\n",
       "      <td>0.547692</td>\n",
       "      <td>0.913920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.094200</td>\n",
       "      <td>0.318340</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.517689</td>\n",
       "      <td>0.571711</td>\n",
       "      <td>0.543361</td>\n",
       "      <td>0.914079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.088400</td>\n",
       "      <td>0.321414</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.519428</td>\n",
       "      <td>0.575173</td>\n",
       "      <td>0.545881</td>\n",
       "      <td>0.914343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.085500</td>\n",
       "      <td>0.325657</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.522799</td>\n",
       "      <td>0.572700</td>\n",
       "      <td>0.546613</td>\n",
       "      <td>0.913920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.080500</td>\n",
       "      <td>0.326878</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.522645</td>\n",
       "      <td>0.570722</td>\n",
       "      <td>0.545626</td>\n",
       "      <td>0.913920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.077500</td>\n",
       "      <td>0.331113</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.521152</td>\n",
       "      <td>0.572700</td>\n",
       "      <td>0.545712</td>\n",
       "      <td>0.914079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.072800</td>\n",
       "      <td>0.336064</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.516854</td>\n",
       "      <td>0.568744</td>\n",
       "      <td>0.541559</td>\n",
       "      <td>0.913444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.070400</td>\n",
       "      <td>0.339053</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.520108</td>\n",
       "      <td>0.569238</td>\n",
       "      <td>0.543566</td>\n",
       "      <td>0.913973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.066900</td>\n",
       "      <td>0.343722</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.521934</td>\n",
       "      <td>0.576657</td>\n",
       "      <td>0.547932</td>\n",
       "      <td>0.913708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.064300</td>\n",
       "      <td>0.345456</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.526009</td>\n",
       "      <td>0.580119</td>\n",
       "      <td>0.551740</td>\n",
       "      <td>0.914079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>0.062100</td>\n",
       "      <td>0.347919</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.514554</td>\n",
       "      <td>0.568249</td>\n",
       "      <td>0.540071</td>\n",
       "      <td>0.912650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>0.059700</td>\n",
       "      <td>0.352430</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.520018</td>\n",
       "      <td>0.571711</td>\n",
       "      <td>0.544641</td>\n",
       "      <td>0.913232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>0.353436</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.520289</td>\n",
       "      <td>0.570722</td>\n",
       "      <td>0.544340</td>\n",
       "      <td>0.913497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.054400</td>\n",
       "      <td>0.358600</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.519568</td>\n",
       "      <td>0.571217</td>\n",
       "      <td>0.544170</td>\n",
       "      <td>0.913179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>0.052600</td>\n",
       "      <td>0.360606</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.522469</td>\n",
       "      <td>0.569238</td>\n",
       "      <td>0.544852</td>\n",
       "      <td>0.913708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>0.051200</td>\n",
       "      <td>0.363639</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.576657</td>\n",
       "      <td>0.548964</td>\n",
       "      <td>0.914184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>0.049600</td>\n",
       "      <td>0.367609</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.524434</td>\n",
       "      <td>0.573195</td>\n",
       "      <td>0.547732</td>\n",
       "      <td>0.913708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>0.047900</td>\n",
       "      <td>0.368593</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.523681</td>\n",
       "      <td>0.574184</td>\n",
       "      <td>0.547771</td>\n",
       "      <td>0.913497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.045800</td>\n",
       "      <td>0.375462</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.520161</td>\n",
       "      <td>0.574184</td>\n",
       "      <td>0.545839</td>\n",
       "      <td>0.913338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>0.043100</td>\n",
       "      <td>0.376310</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.526935</td>\n",
       "      <td>0.575668</td>\n",
       "      <td>0.550225</td>\n",
       "      <td>0.913708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ПОСЛЕ дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     0.3437 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0010 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.5219 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.5767 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.5479 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.9137 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.5073 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   394.2370 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    25.6250 |\n",
      "+-----------------------------+------------+\n",
      "| epoch                       |    82.0000 |\n",
      "+-----------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForMaskedLM,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tabulate\n",
    "\n",
    "\n",
    "# Разделение данных на train и test\n",
    "train_docs, test_docs = train_test_split(docs, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "\n",
    "# Подготовка данных для MLM\n",
    "mlm_texts = [doc.text for doc in train_docs]\n",
    "mlm_dataset = Dataset.from_dict({\"text\": mlm_texts})\n",
    "\n",
    "def tokenize_mlm(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_special_tokens_mask=True\n",
    "    )\n",
    "\n",
    "tokenized_mlm = mlm_dataset.map(tokenize_mlm, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Обучение MLM\n",
    "mlm_model = AutoModelForMaskedLM.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "data_collator_mlm = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=True,\n",
    "    mlm_probability=0.15\n",
    ")\n",
    "\n",
    "training_args_mlm = TrainingArguments(\n",
    "    output_dir=\"./mlm_results\",\n",
    "    overwrite_output_dir=True,\n",
    "    max_steps=3000,\n",
    "    per_device_train_batch_size=16,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./mlm_logs',\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer_mlm = Trainer(\n",
    "    model=mlm_model,\n",
    "    args=training_args_mlm,\n",
    "    train_dataset=tokenized_mlm,\n",
    "    data_collator=data_collator_mlm\n",
    ")\n",
    "\n",
    "trainer_mlm.train()\n",
    "mlm_model.save_pretrained(\"./mlm_model\")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Создаем Dataset\n",
    "dataset = Dataset.from_list(processed_data)\n",
    "dataset.features['ner_tags'].feature.names = label_list\n",
    "splitted_dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42)\n",
    "tokenized_dataset = splitted_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=splitted_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# Инициализация модели для NER\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    \"./mlm_model\",\n",
    "    num_labels=len(label_list),\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Обучение NER\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    max_steps =10000,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    # warmup_steps=500\n",
    ")\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.add_callback(EarlyStoppingCallback(\n",
    "    early_stopping_patience=10,\n",
    "    early_stopping_threshold=0.001\n",
    "))\n",
    "\n",
    "baseline_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ДО дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    baseline_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "final_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ПОСЛЕ дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    final_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также будем ориентироваться на F1-меру.     \n",
    "F1 до обучения: 0.0067       \n",
    "F1 после обучения: 0.5211->0.5479 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация синтетической разметки новостного `lenta-ru-news.csv.gz` корпуса `xlm-roberta-large-finetuned-conll03-english` NER моделью и использование ее для дообучения rubert-tiny2 вместе с основным набором данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирование выборки из 15000 семплов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corus import load_lenta\n",
    "import pandas as pd\n",
    "\n",
    "path = 'lenta-ru-news.csv.gz'\n",
    "records = load_lenta(path)\n",
    "# next(records)\n",
    "\n",
    "data = []\n",
    "for record in records:\n",
    "    if record.topic is None:\n",
    "        continue\n",
    "    data.append({\n",
    "        'title': record.title,\n",
    "        'text': record.text,\n",
    "        'topic': record.topic\n",
    "    })\n",
    "    if len(data) >= 15_000:\n",
    "        break\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разметка текста умной моделью и сохранение в pandas датафрейм"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large-finetuned-conll03-english were not used when initializing XLMRobertaForTokenClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cuda:0\n",
      "  0%|          | 0/15000 [00:00<?, ?it/s]d:\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\models\\xlm_roberta\\modeling_xlm_roberta.py:371: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "  0%|          | 11/15000 [00:01<27:42,  9.01it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "100%|██████████| 15000/15000 [26:55<00:00,  9.29it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "ner_model = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"xlm-roberta-large-finetuned-conll03-english\",\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def process_text(text):\n",
    "    \"\"\"Обрабатывает текст и извлекает N-сущности с индексами\"\"\"\n",
    "    entities = ner_model(text)\n",
    "    return [\n",
    "        {\n",
    "            \"ner_token\": ent[\"word\"],\n",
    "            \"label\": ent[\"entity_group\"],\n",
    "            \"start\": ent[\"start\"],\n",
    "            \"end\": ent[\"end\"]\n",
    "        }\n",
    "        for ent in entities\n",
    "    ]\n",
    "\n",
    "tqdm.pandas()\n",
    "df[\"ner_entities\"] = df[\"text\"].progress_apply(process_text)\n",
    "\n",
    "result_df = pd.DataFrame({\n",
    "    \"text\": df[\"text\"],\n",
    "    \"ner_entities\": df[\"ner_entities\"]\n",
    "})\n",
    "\n",
    "result_df.to_csv(\"ner_dataset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирование размеченного датасета для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 15000/15000 [00:43<00:00, 341.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['tokens', 'ner_tags'],\n",
      "    num_rows: 15000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Загрузка токенизатора\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "\n",
    "def result_df_to_dataset(result_df, label2id):\n",
    "    processed_data = []\n",
    "    \n",
    "    for _, row in tqdm(result_df.iterrows(), total=len(result_df)):\n",
    "        text = row['text']\n",
    "        entities = row['ner_entities']\n",
    "        \n",
    "        # Токенизация текста\n",
    "        tokenized = tokenizer(\n",
    "            text,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=False,\n",
    "            add_special_tokens=False\n",
    "        )\n",
    "        \n",
    "        # Собираем информацию о токенах\n",
    "        tokens_info = []\n",
    "        for token, offset in zip(tokenized.tokens(), tokenized.offset_mapping):\n",
    "            if token in tokenizer.all_special_tokens:\n",
    "                continue\n",
    "            tokens_info.append({\n",
    "                'token': token,\n",
    "                'start': offset[0],\n",
    "                'end': offset[1],\n",
    "                'is_word_start': not token.startswith('##')\n",
    "            })\n",
    "        \n",
    "        # Создаем список меток\n",
    "        token_labels = ['O'] * len(tokens_info)\n",
    "        \n",
    "        # Сортируем сущности по длине (длинные сначала)\n",
    "        sorted_entities = sorted(\n",
    "            entities,\n",
    "            key=lambda x: x['end'] - x['start'],\n",
    "            reverse=True\n",
    "        )\n",
    "        \n",
    "        # Размечаем сущности\n",
    "        for ent in sorted_entities:\n",
    "            ent_start = ent['start']\n",
    "            ent_end = ent['end']\n",
    "            ent_type = ent['label']\n",
    "            \n",
    "            # Ищем пересекающиеся токены\n",
    "            for i, token in enumerate(tokens_info):\n",
    "                if token_labels[i] != 'O':\n",
    "                    continue\n",
    "                \n",
    "                # Проверяем пересечение токена с сущностью\n",
    "                if (token['start'] >= ent_end) or (token['end'] <= ent_start):\n",
    "                    continue\n",
    "                \n",
    "                # Определяем B/I\n",
    "                if token['is_word_start']:\n",
    "                    # Проверяем начало сущности\n",
    "                    if token['start'] == ent_start:\n",
    "                        prefix = 'B-'\n",
    "                    else:\n",
    "                        prefix = 'I-'\n",
    "                else:\n",
    "                    prefix = 'I-'\n",
    "                \n",
    "                # Проверяем существование метки\n",
    "                full_label = f\"{prefix}{ent_type}\"\n",
    "                if full_label not in label2id:\n",
    "                    continue  # Пропускаем неизвестные метки\n",
    "                \n",
    "                token_labels[i] = full_label\n",
    "        \n",
    "        # Конвертируем метки в ID\n",
    "        ner_tags = [label2id.get(label, label2id['O']) for label in token_labels]\n",
    "        \n",
    "        processed_data.append({\n",
    "            'tokens': [t['token'] for t in tokens_info],\n",
    "            'ner_tags': ner_tags\n",
    "        })\n",
    "    \n",
    "    features = dataset.features\n",
    "    new_dataset = Dataset.from_list(processed_data, features=features)\n",
    "    \n",
    "    return new_dataset\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(label_list)}\n",
    "new_dataset = result_df_to_dataset(result_df, label2id)\n",
    "\n",
    "print(new_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Объединение с оригинальным датасетом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "splitted_orig_dataset = dataset.train_test_split(test_size=0.2, shuffle=True, seed=42) if not isinstance(dataset, DatasetDict) else dataset\n",
    "combined_train = concatenate_datasets([\n",
    "    splitted_orig_dataset['train'], \n",
    "    new_dataset \n",
    "])\n",
    "\n",
    "combined_dataset = DatasetDict({\n",
    "    'train': combined_train,\n",
    "    'test': splitted_orig_dataset['test']\n",
    "})\n",
    "\n",
    "print(combined_dataset)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дообучение rubert-tiny2 вместе на новом наборе данных вместе с основным"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at cointegrated/rubert-tiny2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1997691fe3d4859a9e533a63d7d3fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a2d7fd0edbc43499e6096e1716bc437",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:48]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ДО дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     2.4345 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0010 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.0049 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.0381 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.0087 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.0716 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.7654 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   261.3100 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    16.9850 |\n",
      "+-----------------------------+------------+\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='30000' max='30000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [30000/30000 23:41, Epoch 30/31]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.790659</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.152301</td>\n",
       "      <td>0.180020</td>\n",
       "      <td>0.165005</td>\n",
       "      <td>0.805830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.136000</td>\n",
       "      <td>0.636909</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.215755</td>\n",
       "      <td>0.243818</td>\n",
       "      <td>0.228930</td>\n",
       "      <td>0.827469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.107000</td>\n",
       "      <td>0.538011</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.239067</td>\n",
       "      <td>0.283877</td>\n",
       "      <td>0.259552</td>\n",
       "      <td>0.841437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.089600</td>\n",
       "      <td>0.451956</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.283076</td>\n",
       "      <td>0.325915</td>\n",
       "      <td>0.302989</td>\n",
       "      <td>0.861912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.078000</td>\n",
       "      <td>0.401337</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.325571</td>\n",
       "      <td>0.366469</td>\n",
       "      <td>0.344812</td>\n",
       "      <td>0.875721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.068400</td>\n",
       "      <td>0.385799</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.356067</td>\n",
       "      <td>0.397626</td>\n",
       "      <td>0.375701</td>\n",
       "      <td>0.882810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.352127</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.389088</td>\n",
       "      <td>0.430267</td>\n",
       "      <td>0.408643</td>\n",
       "      <td>0.890323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>0.056100</td>\n",
       "      <td>0.357688</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.387986</td>\n",
       "      <td>0.424827</td>\n",
       "      <td>0.405571</td>\n",
       "      <td>0.890323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>0.052200</td>\n",
       "      <td>0.338247</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.408476</td>\n",
       "      <td>0.448071</td>\n",
       "      <td>0.427358</td>\n",
       "      <td>0.895085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>0.047700</td>\n",
       "      <td>0.334893</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.427419</td>\n",
       "      <td>0.471810</td>\n",
       "      <td>0.448519</td>\n",
       "      <td>0.897572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>0.044400</td>\n",
       "      <td>0.317880</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.442458</td>\n",
       "      <td>0.477250</td>\n",
       "      <td>0.459196</td>\n",
       "      <td>0.902492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.339434</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.427596</td>\n",
       "      <td>0.464392</td>\n",
       "      <td>0.445235</td>\n",
       "      <td>0.897995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.332563</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.446673</td>\n",
       "      <td>0.484669</td>\n",
       "      <td>0.464896</td>\n",
       "      <td>0.899265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>0.036600</td>\n",
       "      <td>0.328753</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.442308</td>\n",
       "      <td>0.477745</td>\n",
       "      <td>0.459344</td>\n",
       "      <td>0.901857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>0.035300</td>\n",
       "      <td>0.331799</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.456092</td>\n",
       "      <td>0.490603</td>\n",
       "      <td>0.472719</td>\n",
       "      <td>0.903338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>0.032800</td>\n",
       "      <td>0.334788</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.449433</td>\n",
       "      <td>0.490109</td>\n",
       "      <td>0.468890</td>\n",
       "      <td>0.903127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.335559</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.460201</td>\n",
       "      <td>0.497527</td>\n",
       "      <td>0.478137</td>\n",
       "      <td>0.903815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>0.030200</td>\n",
       "      <td>0.340902</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.461822</td>\n",
       "      <td>0.496538</td>\n",
       "      <td>0.478551</td>\n",
       "      <td>0.904661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>0.029400</td>\n",
       "      <td>0.337584</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.470288</td>\n",
       "      <td>0.500989</td>\n",
       "      <td>0.485153</td>\n",
       "      <td>0.905561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>0.028200</td>\n",
       "      <td>0.355381</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.452206</td>\n",
       "      <td>0.486647</td>\n",
       "      <td>0.468795</td>\n",
       "      <td>0.902862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>0.026900</td>\n",
       "      <td>0.338322</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.476235</td>\n",
       "      <td>0.505440</td>\n",
       "      <td>0.490403</td>\n",
       "      <td>0.908047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>0.026400</td>\n",
       "      <td>0.349701</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.466636</td>\n",
       "      <td>0.501484</td>\n",
       "      <td>0.483433</td>\n",
       "      <td>0.905878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.343173</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.468448</td>\n",
       "      <td>0.502967</td>\n",
       "      <td>0.485094</td>\n",
       "      <td>0.906195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>0.025100</td>\n",
       "      <td>0.349428</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.469577</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.484311</td>\n",
       "      <td>0.905878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>0.345383</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.475701</td>\n",
       "      <td>0.503462</td>\n",
       "      <td>0.489188</td>\n",
       "      <td>0.906830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>0.023900</td>\n",
       "      <td>0.348415</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.471707</td>\n",
       "      <td>0.502967</td>\n",
       "      <td>0.486836</td>\n",
       "      <td>0.906566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.352507</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.473148</td>\n",
       "      <td>0.505440</td>\n",
       "      <td>0.488761</td>\n",
       "      <td>0.906460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>0.023200</td>\n",
       "      <td>0.354400</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.472299</td>\n",
       "      <td>0.505935</td>\n",
       "      <td>0.488539</td>\n",
       "      <td>0.906725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>0.023500</td>\n",
       "      <td>0.354104</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.473073</td>\n",
       "      <td>0.503956</td>\n",
       "      <td>0.488027</td>\n",
       "      <td>0.906566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.353728</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.470370</td>\n",
       "      <td>0.502473</td>\n",
       "      <td>0.485892</td>\n",
       "      <td>0.906619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метрики ПОСЛЕ дообучения:\n",
      "+-----------------------------+------------+\n",
      "| Метрика                     |   Значение |\n",
      "+=============================+============+\n",
      "| eval_loss                   |     0.3383 |\n",
      "+-----------------------------+------------+\n",
      "| eval_model_preparation_time |     0.0010 |\n",
      "+-----------------------------+------------+\n",
      "| eval_precision              |     0.4762 |\n",
      "+-----------------------------+------------+\n",
      "| eval_recall                 |     0.5054 |\n",
      "+-----------------------------+------------+\n",
      "| eval_f1                     |     0.4904 |\n",
      "+-----------------------------+------------+\n",
      "| eval_accuracy               |     0.9080 |\n",
      "+-----------------------------+------------+\n",
      "| eval_runtime                |     0.5023 |\n",
      "+-----------------------------+------------+\n",
      "| eval_samples_per_second     |   398.1370 |\n",
      "+-----------------------------+------------+\n",
      "| eval_steps_per_second       |    25.8790 |\n",
      "+-----------------------------+------------+\n",
      "| epoch                       |    30.3644 |\n",
      "+-----------------------------+------------+\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import Dataset, DatasetDict\n",
    "import tabulate\n",
    "\n",
    "# Загрузка модели и токенизатора\n",
    "model_name = \"cointegrated/rubert-tiny2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(label_list),\n",
    "    id2label={i: label for i, label in enumerate(label_list)},\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        truncation=True,\n",
    "        is_split_into_words=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128\n",
    "    )\n",
    "    \n",
    "    labels = []\n",
    "    for i, label in enumerate(examples[\"ner_tags\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
    "        previous_word_idx = None\n",
    "        label_ids = []\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                label_ids.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                label_ids.append(label[word_idx])\n",
    "            else:\n",
    "                label_ids.append(-100)\n",
    "            previous_word_idx = word_idx\n",
    "        labels.append(label_ids)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = combined_dataset.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=combined_dataset[\"train\"].column_names\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_upd_dataset_results\",\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=3000,\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    max_steps=30000,\n",
    "    logging_steps=1000,\n",
    "    weight_decay=0.01,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    # lr_scheduler_type=\"cosine\",\n",
    "    # warmup_steps=500\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForTokenClassification(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# trainer.add_callback(EarlyStoppingCallback(\n",
    "#     early_stopping_patience=10,\n",
    "#     early_stopping_threshold=0.001,\n",
    "# ))\n",
    "\n",
    "baseline_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ДО дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    baseline_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "final_metrics = trainer.evaluate(tokenized_dataset[\"test\"])\n",
    "print(\"Метрики ПОСЛЕ дообучения:\")\n",
    "print(tabulate.tabulate(\n",
    "    final_metrics.items(),\n",
    "    headers=[\"Метрика\", \"Значение\"],\n",
    "    tablefmt=\"grid\",\n",
    "    floatfmt=\".4f\"\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также будем ориентироваться на F1-меру.     \n",
    "F1 до обучения: 0.0087     \n",
    "F1 после обучения: 0.5211->0.5479->0.4904 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
